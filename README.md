# Bigram Language Model for Name Generation

This project is based on [Andrej Karpathy's Bigram Language Model tutorial](https://www.youtube.com/watch?v=PaCmpygFfXo&t=3001s), modified to generate new names from a dataset of English and Polish names. The model is trained using bigrams (pairs of characters), enabling it to produce realistic names from the combined dataset.

## Project Overview

The goal of this project is to build a Bigram Language Model that predicts the next character in a name given the current character. The model is trained on a set of English and Polish names, and it generates new names based on the learned character transitions. Below, you'll find a step-by-step breakdown of the project and the key components of the code. You will find ready to use code in `notebooks/model.ipynb` file.

## Results

Here are some examples of new names generated by the model:

- junide
- janasah
- cony
- kohin
- tolia
- tee

---
## Code 

### 1. Data Preparation

We begin by reading the list of names from a text file and creating a bigram frequency table for character transitions. We assign an integer index to each character and use a matrix to store the transition counts.

```python
import torch
import numpy

# Read the names from the file
words = open("../names.txt", "r").read().splitlines()

# Initialize a 33x33 matrix to count bigram frequencies
N = torch.zeros((33, 33), dtype=torch.int32)

# Get unique characters from the dataset
characters = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(characters)}  # String to index mapping
stoi['.'] = 0  # Start/End token mapped to index 0
itos = {i:s for s, i in stoi.items()}  # Index to string mapping

# Count bigram occurrences for each pair of characters
for w in words:
    chars = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chars, chars[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        N[ix1, ix2] += 1
```
Here, N stores the transition counts for each character pair, including the start and end of names.


### 2. Visualizing the Bigram Matrix

Next, we visualize the bigram matrix using a heatmap to understand how frequently different character pairs occur.

```python
import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,16))
plt.imshow(N, cmap='Blues')

# Annotate each cell with the character pair and its count
for i in range(33):
    for j in range(33):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')
        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray')

plt.axis('off')
```
This step provides a visual representation of character pair frequencies.


### 3. Generating New Names

Using the bigram probability matrix, we can now generate new names by sampling character transitions until we reach the end token.

```python
P = (N+1).float()  # Convert counts to probabilities with Laplace smoothing
P /= P.sum(1, keepdim=True)  # Normalize the rows

generator = torch.Generator().manual_seed(2147483647)  # For reproducibility

# Generate 20 names
for i in range(20):
    out = []
    ix = 0  # Start with the '.' token
    while True:
        p = P[ix]
        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=generator).item()
        out.append(itos[ix])
        if ix == 0:
            break
    print(''.join(out))
```
This step generates new names by sampling character transitions based on the learned probabilities.



### 4. Model Evaluation with Log Likelihood

To evaluate the model, we calculate the log likelihood of the dataset using the trained bigram model. We also compute the negative log likelihood (NLL) as a loss metric.

```python
log_likelihood = 0.0
n = 0

# Calculate log likelihood for the entire dataset
for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1

print(f'Log likelihood: {log_likelihood:.4f}')
nll = -log_likelihood
print(f'Negative log likelihood: {nll:.4f}')
print(f'Normalized negative log likelihood: {nll/n:.4f}')
```
The negative log likelihood gives an indication of how well the model fits the training data.



### 5. Example of Training Data for Neural Network

Example of input and target sequences (encoded as integer indices) for training a neural network.

```python
xs, ys = [], []

# Prepare training data by converting character pairs into input-output pairs
for w in words[:1]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        xs.append(ix1)
        ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)
```


### 6. Full Dataset Preparation for Training

We now prepare the entire dataset for training, converting all names into sequences of input-output pairs.

```python
xs, ys = [], []

for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        xs.append(ix1)
        ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples: ', num)
```


### 7. Training the Neural Network

The neural network is trained using gradient descent to minimize the loss function.

```python
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((33, 33), generator=g, requires_grad=True)

# Training loop
for k in range(5000):
    learning_rate = -50
    if k > 1000:
        learning_rate += k * 0.005

    xenc = F.one_hot(xs, num_classes=33).float()
    logits = xenc @ W
    counts = logits.exp()
    probs = counts / counts.sum(1, keepdims=True)
    loss = -probs[torch.arange(num), ys].log().mean() + 0.00001 * (W**2).mean()
    print(loss.item())

    W.grad = None
    loss.backward()
    W.data += learning_rate * W.grad
```


### 8. Neural Network Loss Calculation

We calculate the negative log likelihood for each example in the training data using a simple feed-forward neural network.

```python
import torch.nn.functional as F

nlls = torch.zeros(5)
for i in range(5):
    x = xs[i].item()
    y = ys[i].item()
    print("----------------")
    print(f"bigram example {i+1}: {itos[x]}{itos[y]} (index {x}, {y})")
    print("input to the neural net: ", x)
    print("output probabilities from the neural net: ", probs[i])
    print("label (actual next character): ", y)
    p = probs[i, y]
    logp = torch.log(p)
    nll = -logp
    nlls[i] = nll
print("----------------")
print("average negative log likelihood, i.e. loss =  ", nlls.mean().item())
```
This step computes the loss for the neural network based on predicted character probabilities.



### 9. Generating Names with the Trained Model

After training, we use the learned weights to generate new names.

```python
g = torch.Generator().manual_seed(2147483647)

for i in range(10):
    out = []
    ix = 0
    while True:
        xenc = F.one_hot(torch.tensor([ix]), num_classes=33).float()
        logits = xenc @ W
        counts = logits.exp()
        p = counts / counts.sum(1, keepdims=True)

        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        out.append(itos[ix])
        if ix == 0:
            break

    if len(out) > 3:
        print(''.join(out))
```


